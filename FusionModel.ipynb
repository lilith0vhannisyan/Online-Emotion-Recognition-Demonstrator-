{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb09c47-0298-4209-829d-6e0380c60575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────\n",
    "# Cell 1: Imports & Config\n",
    "# ────────────────────────────────────────────────────────────\n",
    "import os, csv, random\n",
    "import torch, torchaudio, whisper\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForAudioClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "# ── Paths & hyperparams ────────────────────────────────────\n",
    "LABELS_CSV  = \"/projects/pdd/IS2025_Podcast_Challenge/Labels/labels_consensus.csv\"\n",
    "AUDIO_DIR   = \"/projects/pdd/IS2025_Podcast_Challenge/Audios\"\n",
    "TRANS_DIR   = \"/projects/pdd/IS2025_Podcast_Challenge/Transcripts\"\n",
    "WAVLM_DIR   = \"wavlm_finetuned_bs16_lr2e-05_wd0.05_ep50_ga2_Lilit\"\n",
    "DEBERTA_DIR = \"saved_deberta_model/DeBERTa_v3_Large_Lr1e-6_gradient6_batch16_stopEpoch9\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ALPHA  = 0.6       # weight for audio\n",
    "BETA   = 1 - ALPHA # weight for text\n",
    "\n",
    "EMO2ID = {\"A\":0,\"S\":1,\"H\":2,\"U\":3,\"F\":4,\"D\":5,\"C\":6,\"N\":7,\"O\":8,\"X\":9}\n",
    "ID2EMO = {v:k for k,v in EMO2ID.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f858e-6293-4122-8523-e9504c60e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────\n",
    "# Cell 2: Load Models (once)\n",
    "# ────────────────────────────────────────────────────────────\n",
    "print(\"Loading models...\")\n",
    "whisper_m = whisper.load_model(\"large\").to(DEVICE).eval()\n",
    "wavlm_fe  = AutoFeatureExtractor.from_pretrained(WAVLM_DIR)\n",
    "wavlm_m   = AutoModelForAudioClassification.from_pretrained(WAVLM_DIR).to(DEVICE).eval()\n",
    "tok       = AutoTokenizer.from_pretrained(DEBERTA_DIR, use_fast=False)\n",
    "deberta_m = AutoModelForSequenceClassification.from_pretrained(DEBERTA_DIR).to(DEVICE).eval()\n",
    "\n",
    "for m in (whisper_m, wavlm_m, deberta_m):\n",
    "    m.requires_grad_(False)\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "print(\"✅ Models ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb13e3-fe50-4287-a223-aaddccb0a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────\n",
    "# Cell 3: Define helper functions\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def audio_probs(wav_path):\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    if sr!=16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    feats = wavlm_fe(wav.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "    logits = wavlm_m(**{k:v.to(DEVICE) for k,v in feats.items()}).logits\n",
    "    return softmax(logits).cpu().numpy().squeeze()\n",
    "\n",
    "def text_probs(text):\n",
    "    inputs = tok(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    logits = deberta_m(**{k:v.to(DEVICE) for k,v in inputs.items()}).logits\n",
    "    return softmax(logits).cpu().numpy().squeeze()\n",
    "\n",
    "def fuse(p_a, p_t):\n",
    "    return ALPHA*p_a + BETA*p_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81fa89-308f-47ad-9202-271315828154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────\n",
    "# Cell 3: Define helper functions\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def audio_probs(wav_path):\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    if sr!=16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    feats = wavlm_fe(wav.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "    logits = wavlm_m(**{k:v.to(DEVICE) for k,v in feats.items()}).logits\n",
    "    return softmax(logits).cpu().numpy().squeeze()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def text_probs(text):\n",
    "    inputs = tok(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    logits = deberta_m(**{k:v.to(DEVICE) for k,v in inputs.items()}).logits\n",
    "    return softmax(logits).cpu().numpy().squeeze()\n",
    "\n",
    "def fuse(p_a, p_t):\n",
    "    return ALPHA*p_a + BETA*p_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c690f309-30fe-41ae-acc6-d17d46d33c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────\n",
    "# Cell 4: Load label rows (Train+Development)\n",
    "# ────────────────────────────────────────────────────────────\n",
    "rows = []\n",
    "with open(LABELS_CSV) as f:\n",
    "    for r in csv.DictReader(f):\n",
    "        if r[\"Split_Set\"].strip() in (\"Train\",\"Development\"):\n",
    "            rows.append(r)\n",
    "\n",
    "print(f\"Found {len(rows)} total samples\")\n",
    "random.shuffle(rows)\n",
    "# (optional) you can subset for a quicker demo, e.g. rows = rows[:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd674928-5b88-4ef7-8521-7d63d07b1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────\n",
    "# Cell 5: Demo on first 3 samples\n",
    "# ────────────────────────────────────────────────────────────\n",
    "for i, row in enumerate(rows[:3], 1):\n",
    "    fn = row[\"FileName\"]\n",
    "    wav = os.path.join(AUDIO_DIR, fn)\n",
    "    txt = os.path.join(TRANS_DIR, fn.replace(\".wav\",\".txt\"))\n",
    "    # get transcript\n",
    "    if os.path.exists(txt):\n",
    "        transcription = open(txt).read().strip()\n",
    "    else:\n",
    "        transcription = whisper_m.transcribe(wav)[\"text\"]\n",
    "    # compute\n",
    "    p_a = audio_probs(wav)\n",
    "    p_t = text_probs(transcription)\n",
    "    p_f = fuse(p_a, p_t)\n",
    "    # report\n",
    "    print(f\"\\n▶️ Sample {i}: {fn}\")\n",
    "    print(\"  • True label   :\", row[\"EmoClass\"])\n",
    "    print(\"  • Audio probs  :\", np.round(p_a,3))\n",
    "    print(\"  • Text probs   :\", np.round(p_t,3))\n",
    "    print(\"  • Fused probs  :\", np.round(p_f,3))\n",
    "    print(\"  • Predicted    :\", ID2EMO[int(p_f.argmax())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b634b-6ccb-4766-a1e5-6fa176365518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────\n",
    "# Cell 6: Full evaluation with periodic progress reports\n",
    "# ────────────────────────────────────────────────────────────\n",
    "correct = 0\n",
    "total   = len(rows)\n",
    "\n",
    "for idx, row in enumerate(rows, 1):\n",
    "    fn   = row[\"FileName\"]\n",
    "    wav  = os.path.join(AUDIO_DIR, fn)\n",
    "    txt  = os.path.join(TRANS_DIR, fn.replace(\".wav\", \".txt\"))\n",
    "    text = open(txt).read().strip() if os.path.exists(txt) \\\n",
    "           else whisper_model.transcribe(wav)[\"text\"]\n",
    "    \n",
    "    p_a  = audio_probs(wav)\n",
    "    p_t  = text_probs(text)\n",
    "    p_f  = ALPHA * p_a + BETA * p_t\n",
    "\n",
    "    pred = int(np.argmax(p_f))\n",
    "    true = EMO2ID[row[\"EmoClass\"].strip()]\n",
    "    correct += (pred == true)\n",
    "\n",
    "    # every 100 files (or at the end) print a mini‐report\n",
    "    if idx % 100 == 0 or idx == total:\n",
    "        acc_so_far = correct / idx\n",
    "        print(f\"→ Processed {idx}/{total} files — \"\n",
    "              f\"current accuracy: {acc_so_far:.4f}\")\n",
    "\n",
    "# final accuracy\n",
    "final_acc = correct / total\n",
    "print(f\"\\n✅ Final Weighted Fusion Accuracy = {final_acc:.4f} \"\n",
    "      f\"({correct}/{total})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cbb8b6-0ba3-495a-944a-32a6d22b8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fa934-5886-4676-9148-03d233adec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper + WavLM + DeBERTa   •   weighted-average fusion  \n",
    "\n",
    "import warnings, os, torch, torchaudio, whisper\n",
    "from IPython.display import Audio, display\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor, AutoModelForAudioClassification,\n",
    "    AutoTokenizer,       AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "# constants\n",
    "WAVLM_DIR    = \"wavlm_finetuned_bs16_lr2e-05_wd0.05_ep50_ga2_Lilit\"\n",
    "DEBERTA_DIR  = \"saved_deberta_model/DeBERTa_v3_Large_Lr1e-6_gradient6_batch16_stopEpoch9\"\n",
    "WHISPER_SIZE = \"large\"               \n",
    "EMOTION_MAP  = {\n",
    "    0:\"A-Anger\", 1:\"S-Sadness\", 2:\"H-Happiness\", 3:\"U-Surprise\",\n",
    "    4:\"F-Fear\",  5:\"D-Disgust\", 6:\"C-Contempt\",  7:\"N-Neutral\",\n",
    "    8:\"O-Other\", 9:\"X-Unknown\"\n",
    "}\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# load models once\n",
    "print(\"⏳ Loading Whisper, WavLM, DeBERTa …\")\n",
    "WHISPER  = whisper.load_model(WHISPER_SIZE).to(DEVICE)\n",
    "FE       = AutoFeatureExtractor.from_pretrained(WAVLM_DIR)\n",
    "WAVLM    = AutoModelForAudioClassification.from_pretrained(WAVLM_DIR).to(DEVICE).eval()\n",
    "TOK      = AutoTokenizer.from_pretrained(DEBERTA_DIR)\n",
    "DEBERTA  = AutoModelForSequenceClassification.from_pretrained(DEBERTA_DIR).to(DEVICE).eval()\n",
    "print(\"✅ Models ready\")\n",
    "\n",
    "softmax = torch.nn.functional.softmax   # shortcut\n",
    "\n",
    "\n",
    "# Models\n",
    "def _wavlm_probs(waveform, sr):\n",
    "    if waveform.dim() == 2 and waveform.size(0) > 1:          # stereo → mono\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # resample to 16 kHz if needed\n",
    "    if sr != 16000:\n",
    "        waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "\n",
    "    inp = FE(\n",
    "        waveform.squeeze().numpy(), sampling_rate=16000,\n",
    "        return_tensors=\"pt\", padding=True, truncation=True,\n",
    "        max_length=16000*10\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = WAVLM(**inp).logits                 \n",
    "        probs  = softmax(logits, dim=-1)             \n",
    "\n",
    "        # collapse batch dimension (average if >1)\n",
    "        probs = probs.mean(dim=0)                 \n",
    "    return probs.cpu().tolist()                    \n",
    "\n",
    "\n",
    "def _deberta_probs(text):\n",
    "   \n",
    "    inp = TOK(text, return_tensors=\"pt\", padding=True,\n",
    "              truncation=True, max_length=512).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        return softmax(DEBERTA(**inp).logits, dim=-1).squeeze().cpu().tolist()\n",
    "\n",
    "\n",
    "# main\n",
    "def predict_emotion(audio_path: str, alpha: float = 0.6, demo: bool = True):\n",
    "    if not os.path.isfile(audio_path):\n",
    "        raise FileNotFoundError(audio_path)\n",
    "\n",
    "    # 1) Whisper → transcript\n",
    "    transcript = WHISPER.transcribe(audio_path)[\"text\"]\n",
    "\n",
    "    # 2) WavLM → audio probabilities\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    p_audio = _wavlm_probs(waveform, sr)\n",
    "\n",
    "    # 3) DeBERTa → text probabilities\n",
    "    p_text = _deberta_probs(transcript)\n",
    "\n",
    "    # 4) Weighted fusion\n",
    "    p_fused = [alpha*pa + (1-alpha)*pt for pa, pt in zip(p_audio, p_text)]\n",
    "\n",
    "    top_audio  = max(range(10), key=p_audio.__getitem__)\n",
    "    top_text   = max(range(10), key=p_text.__getitem__)\n",
    "    top_fused  = max(range(10), key=p_fused.__getitem__)\n",
    "\n",
    "    result = dict(\n",
    "        transcript = transcript,\n",
    "        wavlm_probs   = p_audio,  wavlm_pred   = EMOTION_MAP[top_audio],\n",
    "        deberta_probs = p_text,   deberta_pred = EMOTION_MAP[top_text],\n",
    "        fused_probs   = p_fused,  fused_pred   = EMOTION_MAP[top_fused],\n",
    "        alpha = alpha\n",
    "    )\n",
    "\n",
    "    # ── demo printout ────────────────────────────────────────────\n",
    "    if demo:\n",
    "        display(Audio(audio_path))\n",
    "        print(\"TRANSCRIPT\")\n",
    "        print(transcript, \"\\n\")\n",
    "        def show(name, probs, idx):\n",
    "            print(f\"► {name}\")\n",
    "            for i,p in enumerate(probs):\n",
    "                print(f\"  {EMOTION_MAP[i]:<11}: {p:.4f}\")\n",
    "            print(f\"  TOP-1 → {EMOTION_MAP[idx]}  (p={probs[idx]:.4f})\\n\")\n",
    "\n",
    "        show(\"WavLM (audio)\",  p_audio,  top_audio)\n",
    "        show(\"DeBERTa (text)\", p_text,   top_text)\n",
    "        show(f\"FUSION α={alpha}\", p_fused, top_fused)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204d402-b6fa-40f9-9af9-6efc82c082b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════\n",
    "# Cell 2: Load models + define helpers\n",
    "# ════════════════════════════════════════════════════════════════\n",
    "# 1) Whisper, WavLM & DeBERTa\n",
    "print(\"Loading models…\")\n",
    "whisper_model = whisper.load_model(\"base\").to(DEVICE).eval()\n",
    "\n",
    "wavlm_fe  = AutoFeatureExtractor.from_pretrained(WAVLM_DIR)\n",
    "wavlm_mod = AutoModelForAudioClassification.from_pretrained(WAVLM_DIR)\\\n",
    "               .to(DEVICE).eval()\n",
    "\n",
    "tok       = AutoTokenizer.from_pretrained(DEBERTA_DIR, use_fast=False)\n",
    "deberta   = AutoModelForSequenceClassification.from_pretrained(DEBERTA_DIR)\\\n",
    "               .to(DEVICE).eval()\n",
    "\n",
    "for m in (whisper_model, wavlm_mod, deberta):\n",
    "    m.requires_grad_(False)\n",
    "print(\"  ✓ experts ready\")\n",
    "\n",
    "# 2) FusionHead definition + load weights\n",
    "import torch.nn as nn\n",
    "class FusionHead(nn.Module):\n",
    "    def __init__(self, hidden: int = 32, out_dim: int = 10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(20, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "fusion = FusionHead().to(DEVICE)\n",
    "fusion.load_state_dict(torch.load(FUSION_PT, map_location=DEVICE))\n",
    "fusion.eval()\n",
    "print(\"  ✓ fusion head ready\\n\")\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "# 3) Probability helpers\n",
    "@torch.inference_mode()\n",
    "def audio_probs(path):\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    feats  = wavlm_fe(wav.squeeze().numpy(), sampling_rate=16000,\n",
    "                      return_tensors=\"pt\")\n",
    "    logits = wavlm_mod(**{k:v.to(DEVICE) for k,v in feats.items()}).logits\n",
    "    return softmax(logits).cpu().squeeze().numpy()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def text_probs(text):\n",
    "    toks   = tok(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    logits = deberta(**{k:v.to(DEVICE) for k,v in toks.items()}).logits\n",
    "    return softmax(logits).cpu().squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4c013-05e4-4d25-a008-6ba2cd511ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════\n",
    "# Cell 3: Demonstration on a random file\n",
    "# ════════════════════════════════════════════════════════════════\n",
    "def analyse_random():\n",
    "    # pick & load\n",
    "    files = [f for f in os.listdir(AUDIO_DIR) if f.endswith(\".wav\")]\n",
    "    if not files:\n",
    "        print(\"No .wav files found!\"); return\n",
    "    fn   = random.choice(files)\n",
    "    path = os.path.join(AUDIO_DIR, fn)\n",
    "\n",
    "    # play + plot\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000); sr = 16000\n",
    "\n",
    "    print(f\"\\n🎧 Listening to: {fn}\")\n",
    "    display(Audio(wav.numpy(), rate=sr))\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(wav[0].numpy()); plt.title(\"Waveform\"); plt.show()\n",
    "\n",
    "    # get branch outputs\n",
    "    p_a        = audio_probs(path)\n",
    "    transcript = whisper_model.transcribe(path)[\"text\"]\n",
    "    p_t        = text_probs(transcript)\n",
    "\n",
    "    # fuse via MLP\n",
    "    with torch.inference_mode():\n",
    "        x       = torch.tensor(\n",
    "                    np.concatenate([p_a, p_t]),\n",
    "                    dtype=torch.float32\n",
    "                  ).unsqueeze(0).to(DEVICE)\n",
    "        logits  = fusion(x)\n",
    "        p_fused = softmax(logits).cpu().squeeze().numpy()\n",
    "\n",
    "    # print results\n",
    "    print(f\"Transcript : {transcript[:200]}{'…' if len(transcript)>200 else ''}\\n\")\n",
    "    print(f\"WavLM   → {EMOTIONS[p_a.argmax()]}   |  probs: {np.round(p_a,4)}\")\n",
    "    print(f\"DeBERTa → {EMOTIONS[p_t.argmax()]}   |  probs: {np.round(p_t,4)}\")\n",
    "    print(f\"FUSED   → {EMOTIONS[p_fused.argmax()]}   |  probs: {np.round(p_fused,4)}\")\n",
    "\n",
    "# Run the demo\n",
    "analyse_random()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
